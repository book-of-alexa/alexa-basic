## VUI(Voice User Interface)の特徴
VUI(Voice User Interface)とは、読んで字のごとく「声によるユーザーインタフェース」です。これまで私たちが慣れ親しんできたGUIによるインタフェースとはどのような違いがあるのでしょうか。

### 自分自身がリモコンになる
これまで、何かのデバイスを操作するときにはリモコンを使っていました。スマートフォンが普及した現在では、
リモコンの多くがスマートデバイスやスマート家電という名のもとにスマートフォン上のアプリへと置きかわりました。多くの人がスマートフォンで
外出先から自宅のエアコンを操作したり、外からTVの録画をしたりと、さまざまな利便性を手に入れました。
しかしこれらには、リモコンの時代から乗り越えなければならないイニシャルコストがありました。「マニュアルを読む」という最初のステップです。私たちは新しいデバイスやアプリケーションを入手したときに、まずそのマニュアルを読んで操作方法を覚えます。そして、マニュアルの通りにスマートフォンを設定した後でやっと目的の機能が使えるようになるのです。

VUIが組み込まれたデバイスならば、リモコンやスマートフォンで行っていた操作は自身の声で行うことになります。
リモコンは要らなくなるということです。しかし、リモコンを不要にするための条件があります。それは **「普段我々が利用している会話で操作できる必要がある」** ということです。
VUIが搭載されたデバイスを購入したとしても、その操作をするために特別なキーワードを言わなければいけないのなら、マニュアルを読むことと変わりません。
Amazon Alexa のスマートホームスキル におけるガイダンスUse Your Smart Home Device with Alexaでは **「電気をつけて」「電気を消して」「ボリュームを上げて」「ボリュームを下げて」**　などの汎用性のあるフレーズに限定されているのもこのような理由からでしょう。

カスタムスキルを作る場合でも、これらを意識することはとても重要なファクターになります。
スキルが利用されるシーンを想像しながら、最も自然にユーザーが使うと思われる発話を選択していく必要があるということです。


### 両手が塞がった状況でも利用できる
先述したリモコンやスマートフォンでデバイスを操作するシーンを思い出してください。あなたの手はデバイスを操作するという目的のために占有されています。
VUIは、手で作業をしながら別の作業を可能にするインタフェースです。生活の中で、両手が塞がるシーンを想像してみてください。水仕事をしているとき、重くて大きな荷物を運んでいるとき、自転車を直しているとき、子供をおんぶしているときなど、たくさんのシーンが想像できるでしょう。このようなシーンでVUIは生活を楽しく便利にする可能性を秘めています。

- 手は料理中でベトベトだけど、窓をあけて換気がしたい。「アレクサ、リビングの窓を開けて」
- 重い荷物を持っているけど、ドアがしまっている。「アレクサ、ドアを開けて」
- 子供をおんぶしていて、タクシーが呼べない。「アレクサ、家までのタクシーを呼んで」

ワクワクしてきませんか？

### 距離を埋めるインタフェース
ここでもリモコンの話をしましょう。リモコンやスマートフォンは、遠くにあれば取りに行く必要があります。
VUIは自身の声がリモコンとなり得るのですから、目的を達成するためにリモコンを取りに行く必要が無くなります。電話がかかってきてスマートフォンが遠くにあるシーンを想像してみてください。「アレクサ、電話とって」とその場にいながら通話ができます。
また、部屋の温度を確認したいときに、わざわざ湿度計まで見に行くのはナンセンスでしょう。「アレクサ、今の部屋の温度は？」とだけ聞いて、答えてくれればよいのです。
このように、何かをするときに距離を縮めなければいけないシーンはたくさん
あります。その度に私たちはそれまでの作業を中断してリモコンを操作してからまた作業に戻る、という動作を繰り返してきたわけです。
VUIは、これまで操作するために距離（間隔）が必要だったシーンへ入り込める可能性があります。

### HUMAN CENTRIC(人間中心)
声というものは、当然ながら人が生まれながらに持つものです。人がコミュニケーションするための主要なインタフェースです。
2017年の AWS re:Invent \*1 におけるキーノートで、CTOの Werner Vogels 氏は AWSが初めてAlexaと連携するサービスAlexa for Businessを発表しました。その発表の場でVUIを **HUMAN CENTRIC(人間中心)** と表現したのです。これまでは機械に人間が合わせてきた。これからは機械が人間に合わせる番だと。VUIの本質はこれまでのUXを置き換えることではなく、人と人とのコミュニケーションに近いインタフェースを利用して、テクノロジーにより簡単に触れられるところにあります。そのためには先ほども述べたように、**より自然な会話** そしてより **人間的な発話** が求められているのです。
[https://aws.amazon.com/jp/alexaforbusiness/](https://aws.amazon.com/jp/alexaforbusiness/)

\*1 https://reinvent.awsevents.com/


### アナログと曖昧さのバイアス
音声認識の歴史は古く、1970年代にIBMが先駆けて音声認識技術の研究をはじめました。その後 1995年に Windows 95(Microsoft) が発売され、
世界初のスピーチサポートが搭載されました。スマートフォンでは、2011年にApple社が iPhone4 に初めて Siri(Apple)を
搭載しました。2014年には Cortana(Microsoft)がローンチされました。そして、2014年から現在 Amazon Alexa(Amazon)、
Google Home(Google)、LINE Wave(LINE)、Home Pod(Apple) など、スマートスピーカーと呼ばれる製品の乱立に至ります。声は人によって表現が違い、音のトーンも様々です。音はつまるところ振動なので、同じことを言っていても、振動はそれぞれ違います。この違いはコンピューターにとっては由々しき問題でした。「同じことを言っている」と単純に判断できなかったからです。

この問題を解決する方法として、膨大な量の音のパターンや会話パターンを蓄積して分析するという統計的手法によるアプローチが研究されてきました。そして現在、クラウドの登場で膨大なデータを分析できる機械学習基盤が登場し、AmazonやGoogleがこれまで培ってきたテクノロジーと蓄積してきたデータを分析することによって、音声認識(ASR)の精度は格段に向上しました。しかし私たちが普段行う人と人との会話と比べると、まだまだ不十分な点はいなめません。
VUIはそのような曖昧さと脆弱な基盤の上に成り立っているインタフェースという点を忘れてはなりません。VUIを設計する際には、スキルが活用されるシーンによってその曖昧さを許容できるか、また誤認識する場合があるということを常に意識しながら設計していくことになります。
